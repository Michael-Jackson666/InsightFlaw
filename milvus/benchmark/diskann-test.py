"""
DiskANN å¤šæ•°æ®é›†æ€§èƒ½æµ‹è¯•è„šæœ¬
==============================
æ”¯æŒå¤šç§ ann-benchmarks æ•°æ®é›†çš„ç»Ÿä¸€æµ‹è¯•æ¡†æ¶

ä½¿ç”¨æ–¹æ³•:
    python diskann-test.py --list                    # åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†
    python diskann-test.py --dataset sift            # æµ‹è¯• SIFT-128
    python diskann-test.py --dataset gist            # æµ‹è¯• GIST-960
    python diskann-test.py --dataset sift1b -n 10M   # æµ‹è¯• SIFT1B å­é›†
    python diskann-test.py --dataset sift --download # ä¸‹è½½æ•°æ®é›†
"""

import argparse
import numpy as np
import struct
import subprocess
import time
import os
import sys
from pathlib import Path

# å¯¼å…¥æ•°æ®é›†å®šä¹‰
from datasets import (
    DATASETS,
    VECTOR_PRESETS,
    DATA_DIR,
    list_datasets,
    get_dataset_path,
    get_dataset_info,
    check_dataset_exists,
)

# ================= é…ç½® =================
DEFAULT_URI = "http://localhost:19530"
DEFAULT_BATCH_SIZE = 50000


# ================= æ•°æ®åŠ è½½å‡½æ•° =================
def load_hdf5_dataset(filepath: Path):
    """ä» HDF5 æ–‡ä»¶åŠ è½½æ•°æ®é›† (ann-benchmarks æ ¼å¼)"""
    import h5py
    
    print(f"ğŸ“‚ æ­£åœ¨è¯»å– HDF5 æ•°æ®é›†: {filepath.name}")
    
    with h5py.File(filepath, 'r') as f:
        train = np.array(f['train'])        # åº•åº“å‘é‡
        test = np.array(f['test'])          # æŸ¥è¯¢å‘é‡
        neighbors = np.array(f['neighbors'])  # ground truth
        distances = np.array(f['distances'])  # ground truth è·ç¦»
    
    print(f"   åº•åº“å‘é‡: {train.shape} ({train.nbytes / 1024 / 1024:.1f} MB)")
    print(f"   æŸ¥è¯¢å‘é‡: {test.shape}")
    print(f"   Ground Truth: {neighbors.shape}")
    
    return train, test, neighbors, distances


def read_bvecs_batch(filepath: Path, start_idx: int, count: int, dim: int = 128):
    """ä» bvecs æ–‡ä»¶æ‰¹é‡è¯»å–å‘é‡"""
    vector_size = 4 + dim
    
    with open(filepath, 'rb') as f:
        f.seek(start_idx * vector_size)
        
        vectors = np.zeros((count, dim), dtype=np.float32)
        
        for i in range(count):
            dim_bytes = f.read(4)
            if len(dim_bytes) < 4:
                break
            
            d = struct.unpack('i', dim_bytes)[0]
            assert d == dim, f"Dimension mismatch: expected {dim}, got {d}"
            
            vec_bytes = f.read(dim)
            vectors[i] = np.frombuffer(vec_bytes, dtype=np.uint8).astype(np.float32)
        
        return vectors


def read_bvecs_all(filepath: Path, dim: int = 128):
    """è¯»å–å…¨éƒ¨ bvecs å‘é‡"""
    vector_size = 4 + dim
    file_size = filepath.stat().st_size
    num_vectors = file_size // vector_size
    
    return read_bvecs_batch(filepath, 0, num_vectors, dim)


def read_ivecs(filepath: Path):
    """è¯»å– ivecs æ ¼å¼çš„ ground truth"""
    with open(filepath, 'rb') as f:
        dim = struct.unpack('i', f.read(4))[0]
        f.seek(0)
        
        vector_size = 4 + dim * 4
        file_size = filepath.stat().st_size
        num_vectors = file_size // vector_size
        
        result = np.zeros((num_vectors, dim), dtype=np.int32)
        
        for i in range(num_vectors):
            d = struct.unpack('i', f.read(4))[0]
            result[i] = np.frombuffer(f.read(d * 4), dtype=np.int32)
        
        return result


# ================= ä¸‹è½½å‡½æ•° =================
def download_dataset(dataset_key: str):
    """ä¸‹è½½æŒ‡å®šæ•°æ®é›†"""
    if dataset_key not in DATASETS:
        print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_key}")
        return False
    
    dataset = DATASETS[dataset_key]
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    filepath = get_dataset_path(dataset_key)
    
    if filepath.exists():
        print(f"âœ… æ•°æ®é›†å·²å­˜åœ¨: {filepath.name}")
        return True
    
    url = dataset["url"]
    print(f"\nğŸ“¥ ä¸‹è½½ {dataset['name']}...")
    print(f"   URL: {url}")
    print(f"   å¤§å°: {dataset['size']}")
    
    try:
        if dataset["format"] == "hdf5":
            # ä½¿ç”¨ curl ä¸‹è½½ HDF5
            cmd = ["curl", "-L", "-o", str(filepath), url]
            subprocess.run(cmd, check=True)
            print(f"âœ… ä¸‹è½½å®Œæˆ: {filepath.name}")
            return True
        
        elif dataset["format"] == "bvecs":
            # SIFT1B éœ€è¦ä¸‹è½½å¤šä¸ªæ–‡ä»¶
            print("âš ï¸  SIFT1B æ•°æ®é›†è¾ƒå¤§ (~128GB)ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½:")
            print(f"   cd {DATA_DIR}")
            print(f"   wget {url}")
            print(f"   gunzip {dataset['filename']}.gz")
            print(f"   wget {dataset.get('query_url', '')}")
            print(f"   wget {dataset.get('gnd_url', '')}")
            return False
            
    except subprocess.CalledProcessError as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False
    except FileNotFoundError:
        print("âŒ curl æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£… curl")
        return False


# ================= ä¸»æµ‹è¯•å‡½æ•° =================
def run_benchmark(
    dataset_key: str,
    num_vectors: int = None,
    uri: str = DEFAULT_URI,
    batch_size: int = DEFAULT_BATCH_SIZE,
    hdf5_path: str = None,
):
    """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    from pymilvus import MilvusClient, DataType
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    if hdf5_path:
        # è‡ªå®šä¹‰ HDF5 æ–‡ä»¶
        filepath = Path(hdf5_path)
        xb, xq, gt, _ = load_hdf5_dataset(filepath)
        dimension = xb.shape[1]
        metric_type = "L2"
        collection_name = f"custom_benchmark"
    else:
        dataset = get_dataset_info(dataset_key)
        filepath = get_dataset_path(dataset_key)
        dimension = dataset["dimension"]
        metric_type = dataset["metric"]
        collection_name = f"{dataset_key}_benchmark"
        
        # æ£€æŸ¥æ•°æ®é›†
        if not filepath.exists():
            print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {filepath}")
            print(f"   è¯·å…ˆä¸‹è½½: python diskann-test.py --dataset {dataset_key} --download")
            return
        
        # åŠ è½½æ•°æ®
        if dataset["format"] == "hdf5":
            xb, xq, gt, _ = load_hdf5_dataset(filepath)
            if num_vectors:
                num_vectors = min(num_vectors, len(xb))
                xb = xb[:num_vectors]
            else:
                num_vectors = len(xb)
        
        elif dataset["format"] == "bvecs":
            # SIFT1B
            query_path = DATA_DIR / "bigann_query.bvecs"
            if not query_path.exists():
                print(f"âŒ æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {query_path}")
                return
            
            xq = read_bvecs_all(query_path, dimension)
            gt = None  # TODO: åŠ è½½ ground truth
            xb = None  # å»¶è¿ŸåŠ è½½
            
            if num_vectors is None:
                num_vectors = 1_000_000_000  # é»˜è®¤å…¨éƒ¨
    
    print("\n" + "=" * 70)
    print(f"ğŸš€ DiskANN æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    print(f"   æ•°æ®é›†: {dataset_key if not hdf5_path else hdf5_path}")
    print(f"   å‘é‡æ•°: {num_vectors:,}")
    print(f"   ç»´åº¦: {dimension}")
    print(f"   è·ç¦»ç±»å‹: {metric_type}")
    print("=" * 70)
    
    # è¿æ¥ Milvus
    print("\nğŸ”Œ è¿æ¥ Milvus...")
    try:
        client = MilvusClient(uri=uri)
        print(f"   âœ… å·²è¿æ¥: {uri}")
        print(f"   ğŸ“¦ æœåŠ¡å™¨ç‰ˆæœ¬: {client.get_server_version()}")
    except Exception as e:
        print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿ Milvus æ­£åœ¨è¿è¡Œ")
        return
    
    # åˆ›å»ºé›†åˆ
    if client.has_collection(collection_name):
        print(f"\nâš ï¸  åˆ é™¤å·²å­˜åœ¨çš„é›†åˆ: {collection_name}")
        client.drop_collection(collection_name)
    
    print(f"\nğŸ“¦ åˆ›å»ºé›†åˆ: {collection_name}")
    
    schema = client.create_schema(auto_id=False, enable_dynamic_field=False)
    schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True)
    schema.add_field(field_name="vector", datatype=DataType.FLOAT_VECTOR, dim=dimension)
    
    # DiskANN ç´¢å¼•
    index_params = client.prepare_index_params()
    index_params.add_index(
        field_name="vector",
        index_type="DISKANN",
        metric_type=metric_type,
        index_name="vector_index"
    )
    
    client.create_collection(
        collection_name=collection_name,
        schema=schema,
        index_params=index_params
    )
    print(f"   âœ… é›†åˆåˆ›å»ºæˆåŠŸ (DiskANN, {metric_type})")
    
    # æ’å…¥æ•°æ®
    print("\n" + "=" * 70)
    print(f"ğŸš€ å¼€å§‹æ’å…¥æ•°æ® ({num_vectors:,} æ¡)...")
    print("=" * 70)
    
    insert_start = time.time()
    
    for i in range(0, num_vectors, batch_size):
        end = min(i + batch_size, num_vectors)
        
        if xb is not None:
            batch_vectors = xb[i:end]
        else:
            # bvecs æµå¼è¯»å–
            batch_vectors = read_bvecs_batch(filepath, i, end - i, dimension)
        
        batch_data = [
            {"id": i + j, "vector": batch_vectors[j].tolist()}
            for j in range(len(batch_vectors))
        ]
        
        client.insert(collection_name=collection_name, data=batch_data)
        
        progress = (end / num_vectors) * 100
        elapsed = time.time() - insert_start
        rate = end / elapsed if elapsed > 0 else 0
        eta = (num_vectors - end) / rate if rate > 0 else 0
        
        print(f"   è¿›åº¦: {end:>12,} / {num_vectors:,} ({progress:5.1f}%) | "
              f"{rate:,.0f} vec/s | ETA: {eta/60:.1f} min", end="\r")
    
    insert_time = time.time() - insert_start
    print(f"\nâœ… æ’å…¥å®Œæˆ! è€—æ—¶: {insert_time:.1f}s ({insert_time/60:.1f} min)")
    
    # æ„å»ºç´¢å¼•
    print("\n" + "=" * 70)
    print("ğŸ”¨ æ­£åœ¨æ„å»º DiskANN ç´¢å¼•...")
    print("=" * 70)
    
    index_start = time.time()
    client.flush(collection_name=collection_name)
    
    while True:
        info = client.describe_index(collection_name=collection_name, index_name="vector_index")
        indexed = info.get('indexed_rows', 0)
        total = info.get('total_rows', num_vectors)
        
        if indexed >= num_vectors:
            break
        
        print(f"   ç´¢å¼•è¿›åº¦: {indexed:,} / {total:,} ({indexed/total*100:.1f}%)", end="\r")
        time.sleep(2)
    
    index_time = time.time() - index_start
    print(f"\nâœ… ç´¢å¼•å®Œæˆ! è€—æ—¶: {index_time:.1f}s ({index_time/60:.1f} min)")
    
    # åŠ è½½é›†åˆ
    print("\nğŸ“¥ åŠ è½½é›†åˆ...")
    client.load_collection(collection_name)
    print("   âœ… åŠ è½½å®Œæˆ")
    
    # æ€§èƒ½æµ‹è¯•
    TOP_K = 100
    SEARCH_PARAMS = {"metric_type": metric_type, "params": {"search_list": 150}}
    NUM_QUERIES = min(1000, len(xq))
    
    print("\n" + "=" * 70)
    print("âš¡ æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    
    # é¢„çƒ­
    print("   ğŸ”¥ é¢„çƒ­...")
    for _ in range(10):
        client.search(
            collection_name=collection_name,
            data=[xq[0].tolist()],
            limit=TOP_K,
            search_params=SEARCH_PARAMS
        )
    
    # æ­£å¼æµ‹è¯•
    print("   ğŸ” æ‰§è¡Œæœç´¢...")
    query_vectors = [vec.tolist() for vec in xq[:NUM_QUERIES]]
    
    start_time = time.time()
    results = client.search(
        collection_name=collection_name,
        data=query_vectors,
        limit=TOP_K,
        search_params=SEARCH_PARAMS
    )
    end_time = time.time()
    
    total_time = end_time - start_time
    qps = NUM_QUERIES / total_time
    avg_latency = (total_time / NUM_QUERIES) * 1000
    
    # è®¡ç®—å¬å›ç‡
    recall_rate = None
    if gt is not None:
        recall_count = 0
        for i, hits in enumerate(results):
            result_ids = set([hit['id'] for hit in hits])
            gt_ids = set(gt[i, :TOP_K])
            recall_count += len(result_ids.intersection(gt_ids))
        recall_rate = recall_count / (NUM_QUERIES * TOP_K)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š æµ‹è¯•ç»“æœ")
    print("=" * 70)
    print(f"   {'æ•°æ®é›†':<20} {dataset_key}")
    print(f"   {'å‘é‡æ•°':<20} {num_vectors:,}")
    print(f"   {'ç»´åº¦':<20} {dimension}")
    print(f"   {'QPS':<20} {qps:.2f} queries/sec")
    print(f"   {'å¹³å‡å»¶è¿Ÿ':<20} {avg_latency:.2f} ms")
    if recall_rate:
        print(f"   {'Recall@' + str(TOP_K):<20} {recall_rate:.4f} ({recall_rate*100:.2f}%)")
    print("-" * 70)
    print(f"   {'æ’å…¥è€—æ—¶':<20} {insert_time/60:.1f} min")
    print(f"   {'ç´¢å¼•è€—æ—¶':<20} {index_time/60:.1f} min")
    print("=" * 70)
    
    # search_list å¯¹æ¯”
    print("\nğŸ“ˆ search_list å‚æ•°å¯¹æ¯”:")
    print("-" * 50)
    print(f"{'search_list':<15} {'QPS':<15} {'Recall@100':<15}")
    print("-" * 50)
    
    for sl in [100, 150, 200, 300]:
        params = {"metric_type": metric_type, "params": {"search_list": sl}}
        
        start = time.time()
        res = client.search(
            collection_name=collection_name,
            data=query_vectors[:100],
            limit=TOP_K,
            search_params=params
        )
        elapsed = time.time() - start
        
        recall = "N/A"
        if gt is not None:
            cnt = 0
            for i, hits in enumerate(res):
                result_ids = set([hit['id'] for hit in hits])
                gt_ids = set(gt[i, :TOP_K])
                cnt += len(result_ids.intersection(gt_ids))
            recall = f"{cnt / (100 * TOP_K):.4f}"
        
        print(f"{sl:<15} {100/elapsed:<15.2f} {recall:<15}")
    
    print("-" * 50)
    
    # æ¸…ç†
    print("\nğŸ§¹ æ¸…ç†...")
    client.release_collection(collection_name)
    client.close()
    print("âœ… æµ‹è¯•å®Œæˆ!")


# ================= å…¥å£ =================
def main():
    parser = argparse.ArgumentParser(
        description="DiskANN å¤šæ•°æ®é›†æ€§èƒ½æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python diskann-test.py --list                    # åˆ—å‡ºæ•°æ®é›†
  python diskann-test.py --dataset sift            # æµ‹è¯• SIFT-128
  python diskann-test.py --dataset gist            # æµ‹è¯• GIST-960
  python diskann-test.py --dataset sift1b -n 10M   # SIFT1B å­é›†
  python diskann-test.py --dataset sift --download # ä¸‹è½½æ•°æ®é›†
        """
    )
    
    parser.add_argument("--list", "-l", action="store_true",
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†")
    parser.add_argument("--dataset", "-d", type=str,
                       help="é€‰æ‹©æ•°æ®é›† (sift, gist, glove-100, sift1b ç­‰)")
    parser.add_argument("--download", action="store_true",
                       help="ä¸‹è½½æŒ‡å®šæ•°æ®é›†")
    parser.add_argument("-n", "--vectors", type=str,
                       help="å‘é‡æ•°é‡ (1M, 10M, 100M, 1B)")
    parser.add_argument("--hdf5", type=str,
                       help="ä½¿ç”¨è‡ªå®šä¹‰ HDF5 æ–‡ä»¶")
    parser.add_argument("--uri", type=str, default=DEFAULT_URI,
                       help="Milvus æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE,
                       help="æ’å…¥æ‰¹æ¬¡å¤§å°")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ•°æ®é›†
    if args.list:
        list_datasets()
        return
    
    # ä¸‹è½½æ•°æ®é›†
    if args.download:
        if not args.dataset:
            print("âŒ è¯·æŒ‡å®šæ•°æ®é›†: --dataset <ID>")
            return
        download_dataset(args.dataset)
        return
    
    # æ£€æŸ¥å‚æ•°
    if not args.dataset and not args.hdf5:
        parser.print_help()
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨æ•°æ®é›†")
        return
    
    # è§£æå‘é‡æ•°é‡
    num_vectors = None
    if args.vectors:
        num_vectors = VECTOR_PRESETS.get(args.vectors.upper())
        if num_vectors is None:
            try:
                num_vectors = int(args.vectors)
            except ValueError:
                print(f"âŒ æ— æ•ˆçš„å‘é‡æ•°é‡: {args.vectors}")
                return
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import h5py
    except ImportError:
        print("âŒ ç¼ºå°‘ h5pyï¼Œè¯·å®‰è£…: pip install h5py")
        return
    
    # è¿è¡Œæµ‹è¯•
    run_benchmark(
        dataset_key=args.dataset,
        num_vectors=num_vectors,
        uri=args.uri,
        batch_size=args.batch_size,
        hdf5_path=args.hdf5,
    )


if __name__ == "__main__":
    main()
