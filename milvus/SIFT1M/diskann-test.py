"""
SIFT1M DiskANN æ€§èƒ½æµ‹è¯•è„šæœ¬
============================
ç¯å¢ƒ: macOS + Docker Milvus Standalone
æ•°æ®é›†: SIFT1M (100ä¸‡æ¡ 128ç»´å‘é‡)

æµ‹è¯•æŒ‡æ ‡:
- QPS (æ¯ç§’æŸ¥è¯¢æ•°)
- Latency (å¹³å‡å»¶è¿Ÿ)  
- Recall@K (å¬å›ç‡)
"""

import numpy as np
import time
import os

# ================= é…ç½®åŒºåŸŸ =================
URI = "http://localhost:19530"  # Milvus Standalone åœ°å€
COLLECTION_NAME = "sift1m_diskann_test"
DIMENSION = 128  # SIFT æ•°æ®é›†æ˜¯ 128 ç»´

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç¡®ä¿ç›¸å¯¹è·¯å¾„æ­£ç¡®
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
HDF5_PATH = os.path.join(SCRIPT_DIR, "sift-128-euclidean.hdf5")

# ================= å·¥å…·å‡½æ•° =================
def load_hdf5_dataset(filepath):
    """ä» HDF5 æ–‡ä»¶åŠ è½½ SIFT æ•°æ®é›† (ann-benchmarks æ ¼å¼)"""
    import h5py
    
    with h5py.File(filepath, 'r') as f:
        # ann-benchmarks æ ¼å¼çš„ key
        train = np.array(f['train'])      # åº•åº“å‘é‡
        test = np.array(f['test'])        # æŸ¥è¯¢å‘é‡
        neighbors = np.array(f['neighbors'])  # ground truth (æœ€è¿‘é‚» ID)
        distances = np.array(f['distances'])  # ground truth è·ç¦»
    
    return train, test, neighbors, distances

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    try:
        import h5py
        return True
    except ImportError:
        print("âŒ ç¼ºå°‘ h5py åº“ï¼Œè¯·å…ˆå®‰è£…:")
        print("   pip install h5py")
        return False

def check_dataset():
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(HDF5_PATH):
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {HDF5_PATH}")
        print("\nè¯·å…ˆä¸‹è½½æ•°æ®é›†:")
        print(f"   cd {SCRIPT_DIR}")
        print('   curl -L -o sift-128-euclidean.hdf5 "https://ann-benchmarks.com/sift-128-euclidean.hdf5"')
        return False
    return True

# ================= ä¸»ç¨‹åº =================
def main():
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # æ£€æŸ¥æ•°æ®é›†
    if not check_dataset():
        return
    
    # å¯¼å…¥ pymilvus (æ”¾åœ¨è¿™é‡Œé¿å…åœ¨æ£€æŸ¥å¤±è´¥æ—¶æŠ¥é”™)
    from pymilvus import MilvusClient
    
    # 1. åŠ è½½æ•°æ®åˆ°å†…å­˜
    print("=" * 60)
    print("ğŸ“‚ æ­£åœ¨è¯»å– SIFT1M æ•°æ®é›† (HDF5 æ ¼å¼)...")
    print("=" * 60)
    
    xb, xq, gt, gt_distances = load_hdf5_dataset(HDF5_PATH)

    print(f"   åº•åº“æ•°æ®: {xb.shape} ({xb.nbytes / 1024 / 1024:.1f} MB)")
    print(f"   æŸ¥è¯¢æ•°æ®: {xq.shape}")
    print(f"   æ ‡å‡†ç­”æ¡ˆ: {gt.shape}")

    # 2. åˆå§‹åŒ– Milvus
    print("\nğŸ”Œ è¿æ¥ Milvus Standalone...")
    try:
        client = MilvusClient(uri=URI)
        print(f"   âœ… å·²è¿æ¥: {URI}")
        print(f"   ğŸ“¦ æœåŠ¡å™¨ç‰ˆæœ¬: {client.get_server_version()}")
    except Exception as e:
        print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿ Milvus Docker å®¹å™¨æ­£åœ¨è¿è¡Œ:")
        print("   docker ps | grep milvus")
        return

    # é‡å»ºé›†åˆ
    if client.has_collection(COLLECTION_NAME):
        print(f"\nâš ï¸ åˆ é™¤å·²å­˜åœ¨çš„é›†åˆ: {COLLECTION_NAME}")
        client.drop_collection(COLLECTION_NAME)

    # ä½¿ç”¨ Schema æ–¹å¼åˆ›å»ºé›†åˆï¼Œæ‰‹åŠ¨æŒ‡å®š ID ä»¥åŒ¹é… ground truth
    from pymilvus import DataType
    
    print(f"\nğŸ“¦ åˆ›å»ºé›†åˆ: {COLLECTION_NAME}")
    
    # auto_id=False: æ‰‹åŠ¨æŒ‡å®š IDï¼Œç¡®ä¿ä¸ ground truth ä¸­çš„è¡Œå·ä¸€è‡´
    schema = client.create_schema(auto_id=False, enable_dynamic_field=False)
    schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True)
    schema.add_field(field_name="vector", datatype=DataType.FLOAT_VECTOR, dim=DIMENSION)
    
    # å…ˆå‡†å¤‡ DiskANN ç´¢å¼•å‚æ•°
    index_params = client.prepare_index_params()
    index_params.add_index(
        field_name="vector",
        index_type="DISKANN", 
        metric_type="L2",
        index_name="vector_index"
    )
    
    # åˆ›å»ºé›†åˆæ—¶åŒæ—¶æŒ‡å®šç´¢å¼•
    client.create_collection(
        collection_name=COLLECTION_NAME,
        schema=schema,
        index_params=index_params  # ç›´æ¥ä½¿ç”¨ DiskANN ç´¢å¼•
    )
    print("   âœ… é›†åˆåˆ›å»ºæˆåŠŸ (ä½¿ç”¨ DiskANN ç´¢å¼•)")
    print("   â„¹ï¸  ä½¿ç”¨æ‰‹åŠ¨ ID (0 ~ 999999) ä»¥åŒ¹é… ground truth")

    # 3. æ’å…¥æ•°æ® (åˆ†æ‰¹æ’å…¥ï¼ŒåŒ…å«æ‰‹åŠ¨ ID)
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹æ’å…¥æ•°æ® (1M æ¡)...")
    print("=" * 60)
    
    batch_size = 10000
    total_count = len(xb)
    insert_start = time.time()

    for i in range(0, total_count, batch_size):
        end = min(i + batch_size, total_count)
        # è½¬æ¢ numpy æ•°ç»„ä¸º listï¼ŒåŒ…å«æ‰‹åŠ¨æŒ‡å®šçš„ ID (è¡Œå·)
        batch_data = [
            {"id": i + j, "vector": xb[i + j].tolist()} 
            for j in range(end - i)
        ]
        client.insert(collection_name=COLLECTION_NAME, data=batch_data)
        progress = (end / total_count) * 100
        print(f"   è¿›åº¦: {end:>7}/{total_count} ({progress:.1f}%)", end="\r")

    insert_time = time.time() - insert_start
    print(f"\nâœ… æ•°æ®æ’å…¥å®Œæˆ! è€—æ—¶: {insert_time:.2f} ç§’")
    print(f"   æ’å…¥é€Ÿåº¦: {total_count / insert_time:.0f} æ¡/ç§’")

    # 4. åˆ·æ–°å¹¶ç­‰å¾…ç´¢å¼•æ„å»º
    print("\n" + "=" * 60)
    print("ğŸ”¨ æ­£åœ¨æ„å»º DiskANN ç´¢å¼•...")
    print("=" * 60)
    print("   â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    start_idx = time.time()
    client.flush(collection_name=COLLECTION_NAME)
    
    # ç­‰å¾…ç´¢å¼•æ„å»ºå®Œæˆ
    import time as t
    while True:
        index_info = client.describe_index(collection_name=COLLECTION_NAME, index_name="vector_index")
        indexed_rows = index_info.get('indexed_rows', 0)
        total_rows = index_info.get('total_rows', total_count)
        if indexed_rows >= total_count:
            break
        print(f"   ç´¢å¼•è¿›åº¦: {indexed_rows}/{total_rows}", end="\r")
        t.sleep(2)
    
    index_time = time.time() - start_idx
    print(f"\nâœ… ç´¢å¼•æ„å»ºå®Œæˆ! è€—æ—¶: {index_time:.2f} ç§’")

    # 5. åŠ è½½é›†åˆ
    print("\nğŸ“¥ åŠ è½½é›†åˆåˆ°å†…å­˜...")
    client.load_collection(COLLECTION_NAME)
    print("âœ… é›†åˆå·²åŠ è½½")

    # ================= æ€§èƒ½æµ‹è¯•ä¸å¬å›ç‡è®¡ç®— =================
    # å®šä¹‰æœç´¢å‚æ•°
    # æ³¨æ„: DiskANN è¦æ±‚ search_list >= limit (TopK)
    TOP_K = 100  # SIFT æ ‡å‡†é€šå¸¸è®¡ç®— Top 100 çš„å¬å›ç‡
    SEARCH_PARAMS = {
        "metric_type": "L2", 
        "params": {"search_list": 150}  # å¿…é¡» >= TOP_Kï¼Œè¶Šå¤§å¬å›ç‡è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢
    }
    NUM_QUERIES = 1000  # æµ‹è¯•æŸ¥è¯¢æ•°é‡

    print("\n" + "=" * 60)
    print(f"âš¡ å¼€å§‹æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    print(f"   TopK: {TOP_K}")
    print(f"   search_list: {SEARCH_PARAMS['params']['search_list']}")
    print(f"   æµ‹è¯•æŸ¥è¯¢æ•°: {NUM_QUERIES}")
    print(f"   âš ï¸  æ³¨æ„: DiskANN è¦æ±‚ search_list >= TopK")

    # é¢„çƒ­ (Warmup) - è®©ç³»ç»ŸåŠ è½½ç¼“å­˜
    print("\n   ğŸ”¥ é¢„çƒ­ä¸­...")
    for _ in range(10):
        client.search(
            collection_name=COLLECTION_NAME, 
            data=[xq[0].tolist()], 
            limit=TOP_K, 
            search_params=SEARCH_PARAMS
        )
    print("   âœ… é¢„çƒ­å®Œæˆ")

    # æ­£å¼æµ‹è¯•
    print("\n   ğŸ” æ‰§è¡Œæœç´¢æµ‹è¯•...")
    query_vectors = [vec.tolist() for vec in xq[:NUM_QUERIES]]
    
    start_time = time.time()
    results = client.search(
        collection_name=COLLECTION_NAME,
        data=query_vectors,
        limit=TOP_K,
        search_params=SEARCH_PARAMS
    )
    end_time = time.time()

    # ================= ç»“æœåˆ†æ =================
    total_queries = len(results)
    total_time = end_time - start_time
    qps = total_queries / total_time
    avg_latency = (total_time / total_queries) * 1000  # æ¯«ç§’

    # è®¡ç®—å¬å›ç‡ (Recall@K)
    recall_count = 0
    for i, hits in enumerate(results):
        # è·å– Milvus è¿”å›çš„ ID åˆ—è¡¨
        result_ids = set([hit['id'] for hit in hits])
        # è·å–æ ‡å‡†ç­”æ¡ˆçš„ ID åˆ—è¡¨ (å–å‰ TopK)
        ground_truth_ids = set(gt[i, :TOP_K])
        
        # è®¡ç®—äº¤é›†
        intersection = result_ids.intersection(ground_truth_ids)
        recall_count += len(intersection)

    recall_rate = recall_count / (total_queries * TOP_K)

    # æ‰“å°ç»“æœæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ")
    print("=" * 60)
    print(f"   {'æŒ‡æ ‡':<20} {'ç»“æœ':<20}")
    print("-" * 60)
    print(f"   {'QPS (ååé‡)':<20} {qps:.2f} queries/sec")
    print(f"   {'å¹³å‡å»¶è¿Ÿ':<20} {avg_latency:.2f} ms/query")
    print(f"   {'Recall@' + str(TOP_K):<20} {recall_rate:.4f} ({recall_rate*100:.2f}%)")
    print("-" * 60)
    print(f"   {'æ€»æŸ¥è¯¢æ•°':<20} {total_queries}")
    print(f"   {'æ€»è€—æ—¶':<20} {total_time:.2f} s")
    print("=" * 60)

    # ä¸åŒ search_list å¯¹æ¯”æµ‹è¯•
    # æ³¨æ„: search_list å¿…é¡» >= TOP_K (100)
    print("\nğŸ“ˆ search_list å‚æ•°å¯¹æ¯”æµ‹è¯• (search_list >= TopK):")
    print("-" * 50)
    print(f"{'search_list':<15} {'QPS':<15} {'Recall@100':<15}")
    print("-" * 50)
    
    for sl in [100, 150, 200, 300]:  # æ‰€æœ‰å€¼éƒ½ >= TOP_K (100)
        params = {"metric_type": "L2", "params": {"search_list": sl}}
        
        start = time.time()
        res = client.search(
            collection_name=COLLECTION_NAME,
            data=query_vectors[:100],  # ç”¨ 100 ä¸ªæŸ¥è¯¢å¿«é€Ÿæµ‹è¯•
            limit=TOP_K,
            search_params=params
        )
        elapsed = time.time() - start
        
        # è®¡ç®—å¬å›ç‡
        recall = 0
        for i, hits in enumerate(res):
            result_ids = set([hit['id'] for hit in hits])
            ground_truth_ids = set(gt[i, :TOP_K])
            recall += len(result_ids.intersection(ground_truth_ids))
        recall_pct = recall / (100 * TOP_K)
        
        qps_test = 100 / elapsed
        print(f"{sl:<15} {qps_test:<15.2f} {recall_pct:<15.4f}")
    
    print("-" * 50)
    print("ğŸ’¡ ç»“è®º: search_list è¶Šå¤§ï¼Œå¬å›ç‡è¶Šé«˜ï¼Œä½† QPS ä¼šé™ä½")

    # æ¸…ç†ï¼ˆå¯é€‰ï¼‰
    print("\nğŸ§¹ æ¸…ç†èµ„æº...")
    client.release_collection(COLLECTION_NAME)
    # client.drop_collection(COLLECTION_NAME)  # å–æ¶ˆæ³¨é‡Šä»¥åˆ é™¤é›†åˆ
    client.close()
    print("âœ… æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()