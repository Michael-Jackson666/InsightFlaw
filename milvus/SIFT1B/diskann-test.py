"""
SIFT1B DiskANN æ€§èƒ½æµ‹è¯•è„šæœ¬
============================
ç¯å¢ƒ: macOS + Docker Milvus Standalone
æ•°æ®é›†: SIFT1B (10äº¿æ¡ 128ç»´å‘é‡)

æµ‹è¯•æŒ‡æ ‡:
- QPS (æ¯ç§’æŸ¥è¯¢æ•°)
- Latency (å¹³å‡å»¶è¿Ÿ)  
- Recall@K (å¬å›ç‡)

ä½¿ç”¨æ–¹æ³•:
    python diskann-test.py              # é»˜è®¤ä½¿ç”¨å…¨éƒ¨æ•°æ®
    python diskann-test.py -n 10M       # ä½¿ç”¨ 1000 ä¸‡å‘é‡æµ‹è¯•
    python diskann-test.py -n 100M      # ä½¿ç”¨ 1 äº¿å‘é‡æµ‹è¯•
    python diskann-test.py -n 1B        # ä½¿ç”¨ 10 äº¿å‘é‡æµ‹è¯•
    python diskann-test.py --hdf5 xxx   # ä½¿ç”¨ HDF5 æ ¼å¼æ•°æ®é›†
"""

import argparse
import numpy as np
import struct
import time
import os

# ================= é…ç½®åŒºåŸŸ =================
URI = "http://localhost:19530"  # Milvus Standalone åœ°å€
COLLECTION_NAME = "sift1b_diskann_test"
DIMENSION = 128  # SIFT æ•°æ®é›†æ˜¯ 128 ç»´
BATCH_SIZE = 50000  # æ’å…¥æ‰¹æ¬¡å¤§å°

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(SCRIPT_DIR, "data")

# æ•°æ®æ–‡ä»¶è·¯å¾„
BVECS_BASE_PATH = os.path.join(DATA_DIR, "bigann_base.bvecs")
BVECS_QUERY_PATH = os.path.join(DATA_DIR, "bigann_query.bvecs")
GND_DIR = os.path.join(DATA_DIR, "bigann_gnd")

# å‘é‡æ•°é‡é¢„è®¾
PRESETS = {
    "10M": 10_000_000,
    "100M": 100_000_000,
    "500M": 500_000_000,
    "1B": 1_000_000_000,
}


# ================= æ•°æ®åŠ è½½å‡½æ•° =================
def load_hdf5_dataset(filepath):
    """ä» HDF5 æ–‡ä»¶åŠ è½½æ•°æ®é›† (ann-benchmarks æ ¼å¼)"""
    import h5py
    
    print(f"ğŸ“‚ æ­£åœ¨è¯»å– HDF5 æ•°æ®é›†: {os.path.basename(filepath)}")
    
    with h5py.File(filepath, 'r') as f:
        train = np.array(f['train'])      # åº•åº“å‘é‡
        test = np.array(f['test'])        # æŸ¥è¯¢å‘é‡
        neighbors = np.array(f['neighbors'])  # ground truth
        distances = np.array(f['distances'])  # ground truth è·ç¦»
    
    return train, test, neighbors, distances


def read_bvecs_batch(filepath, start_idx, count, dim=128):
    """
    ä» bvecs æ–‡ä»¶æ‰¹é‡è¯»å–å‘é‡
    
    bvecs æ ¼å¼: æ¯ä¸ªå‘é‡ = [dim (4 bytes int)] + [dim * uint8 values]
    """
    vector_size = 4 + dim  # 4 bytes for dimension + dim bytes for vector
    
    with open(filepath, 'rb') as f:
        f.seek(start_idx * vector_size)
        
        vectors = np.zeros((count, dim), dtype=np.float32)
        
        for i in range(count):
            dim_bytes = f.read(4)
            if len(dim_bytes) < 4:
                break
            
            d = struct.unpack('i', dim_bytes)[0]
            assert d == dim, f"Dimension mismatch: expected {dim}, got {d}"
            
            vec_bytes = f.read(dim)
            vectors[i] = np.frombuffer(vec_bytes, dtype=np.uint8).astype(np.float32)
        
        return vectors


def read_bvecs_query(filepath, dim=128):
    """è¯»å–å…¨éƒ¨æŸ¥è¯¢å‘é‡"""
    vector_size = 4 + dim
    file_size = os.path.getsize(filepath)
    num_vectors = file_size // vector_size
    
    return read_bvecs_batch(filepath, 0, num_vectors, dim)


def read_ivecs(filepath):
    """è¯»å– ivecs æ ¼å¼çš„ ground truth"""
    with open(filepath, 'rb') as f:
        # è¯»å–ç¬¬ä¸€ä¸ªå‘é‡çš„ç»´åº¦
        dim = struct.unpack('i', f.read(4))[0]
        f.seek(0)
        
        # è®¡ç®—å‘é‡æ•°é‡
        vector_size = 4 + dim * 4
        file_size = os.path.getsize(filepath)
        num_vectors = file_size // vector_size
        
        result = np.zeros((num_vectors, dim), dtype=np.int32)
        
        for i in range(num_vectors):
            d = struct.unpack('i', f.read(4))[0]
            result[i] = np.frombuffer(f.read(d * 4), dtype=np.int32)
        
        return result


def get_ground_truth(num_vectors):
    """æ ¹æ®å‘é‡æ•°é‡é€‰æ‹©å¯¹åº”çš„ ground truth æ–‡ä»¶"""
    # BigANN æä¾›äº†ä¸åŒè§„æ¨¡çš„ ground truth
    gnd_files = {
        10_000_000: "idx_10M.ivecs",
        100_000_000: "idx_100M.ivecs",
        500_000_000: "idx_500M.ivecs",
        1_000_000_000: "idx_1000M.ivecs",
    }
    
    # æ‰¾åˆ°æœ€æ¥è¿‘çš„ ground truth
    for size, filename in sorted(gnd_files.items()):
        if num_vectors <= size:
            filepath = os.path.join(GND_DIR, filename)
            if os.path.exists(filepath):
                return read_ivecs(filepath)
            # å°è¯• gnd å­ç›®å½•
            filepath = os.path.join(GND_DIR, "gnd", filename)
            if os.path.exists(filepath):
                return read_ivecs(filepath)
    
    return None


# ================= ä¾èµ–æ£€æŸ¥ =================
def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    try:
        import h5py
        return True
    except ImportError:
        print("âš ï¸  h5py æœªå®‰è£…ï¼ŒHDF5 æ ¼å¼ä¸å¯ç”¨")
        print("   pip install h5py")
        return False


def check_bvecs_dataset():
    """æ£€æŸ¥ bvecs æ•°æ®é›†æ˜¯å¦å­˜åœ¨"""
    if os.path.exists(BVECS_BASE_PATH):
        file_size = os.path.getsize(BVECS_BASE_PATH) / (1024**3)
        print(f"âœ… æ‰¾åˆ°åº•åº“æ–‡ä»¶: bigann_base.bvecs ({file_size:.1f} GB)")
        return True
    return False


def check_query_dataset():
    """æ£€æŸ¥æŸ¥è¯¢æ•°æ®é›†"""
    if os.path.exists(BVECS_QUERY_PATH):
        return True
    return False


# ================= ä¸»ç¨‹åº =================
def main():
    parser = argparse.ArgumentParser(description="SIFT1B DiskANN æ€§èƒ½æµ‹è¯•")
    parser.add_argument("-n", "--vectors", type=str, default="1B",
                       help="å‘é‡æ•°é‡ (10M, 100M, 500M, 1B)")
    parser.add_argument("--hdf5", type=str, default=None,
                       help="ä½¿ç”¨ HDF5 æ ¼å¼æ•°æ®é›†")
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE,
                       help="æ’å…¥æ‰¹æ¬¡å¤§å°")
    args = parser.parse_args()
    
    # è§£æå‘é‡æ•°é‡
    num_vectors = PRESETS.get(args.vectors.upper())
    if num_vectors is None:
        try:
            num_vectors = int(args.vectors)
        except ValueError:
            print(f"âŒ æ— æ•ˆçš„å‘é‡æ•°é‡: {args.vectors}")
            print(f"   å¯ç”¨é¢„è®¾: {list(PRESETS.keys())}")
            return
    
    print("=" * 70)
    print(f"ğŸš€ SIFT1B DiskANN æ€§èƒ½æµ‹è¯•")
    print(f"   ç›®æ ‡å‘é‡æ•°: {num_vectors:,}")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®
    if args.hdf5:
        # ä½¿ç”¨ HDF5 æ ¼å¼
        if not os.path.exists(args.hdf5):
            print(f"âŒ HDF5 æ–‡ä»¶ä¸å­˜åœ¨: {args.hdf5}")
            return
        
        xb, xq, gt, _ = load_hdf5_dataset(args.hdf5)
        num_vectors = min(num_vectors, len(xb))
        xb = xb[:num_vectors]
        
    else:
        # ä½¿ç”¨ bvecs æ ¼å¼
        if not check_bvecs_dataset():
            print(f"âŒ åº•åº“æ–‡ä»¶ä¸å­˜åœ¨: {BVECS_BASE_PATH}")
            print("\nè¯·å…ˆä¸‹è½½æ•°æ®é›†:")
            print(f"   cd {DATA_DIR}")
            print('   wget ftp://ftp.irisa.fr/local/texmex/corpus/bigann_base.bvecs.gz')
            print('   gunzip bigann_base.bvecs.gz')
            return
        
        if not check_query_dataset():
            print(f"âŒ æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {BVECS_QUERY_PATH}")
            print("\nè¯·å…ˆä¸‹è½½æŸ¥è¯¢å‘é‡:")
            print(f"   cd {DATA_DIR}")
            print('   wget ftp://ftp.irisa.fr/local/texmex/corpus/bigann_query.bvecs.gz')
            print('   gunzip bigann_query.bvecs.gz')
            return
        
        # è¯»å–æŸ¥è¯¢å‘é‡
        print("\nğŸ“‚ æ­£åœ¨è¯»å–æŸ¥è¯¢å‘é‡...")
        xq = read_bvecs_query(BVECS_QUERY_PATH)
        print(f"   æŸ¥è¯¢å‘é‡: {xq.shape}")
        
        # è¯»å– ground truth
        gt = get_ground_truth(num_vectors)
        if gt is not None:
            print(f"   Ground Truth: {gt.shape}")
        else:
            print("   âš ï¸  æœªæ‰¾åˆ° Ground Truthï¼Œè·³è¿‡å¬å›ç‡è®¡ç®—")
        
        xb = None  # å»¶è¿ŸåŠ è½½åº•åº“å‘é‡
    
    # å¯¼å…¥ pymilvus
    from pymilvus import MilvusClient, DataType
    
    # è¿æ¥ Milvus
    print("\nğŸ”Œ è¿æ¥ Milvus Standalone...")
    try:
        client = MilvusClient(uri=URI)
        print(f"   âœ… å·²è¿æ¥: {URI}")
        print(f"   ğŸ“¦ æœåŠ¡å™¨ç‰ˆæœ¬: {client.get_server_version()}")
    except Exception as e:
        print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿ Milvus Docker å®¹å™¨æ­£åœ¨è¿è¡Œ:")
        print("   docker ps | grep milvus")
        return
    
    # åˆ›å»ºé›†åˆ
    if client.has_collection(COLLECTION_NAME):
        print(f"\nâš ï¸  åˆ é™¤å·²å­˜åœ¨çš„é›†åˆ: {COLLECTION_NAME}")
        client.drop_collection(COLLECTION_NAME)
    
    print(f"\nğŸ“¦ åˆ›å»ºé›†åˆ: {COLLECTION_NAME}")
    
    schema = client.create_schema(auto_id=False, enable_dynamic_field=False)
    schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True)
    schema.add_field(field_name="vector", datatype=DataType.FLOAT_VECTOR, dim=DIMENSION)
    
    # DiskANN ç´¢å¼•
    index_params = client.prepare_index_params()
    index_params.add_index(
        field_name="vector",
        index_type="DISKANN",
        metric_type="L2",
        index_name="vector_index"
    )
    
    client.create_collection(
        collection_name=COLLECTION_NAME,
        schema=schema,
        index_params=index_params
    )
    print("   âœ… é›†åˆåˆ›å»ºæˆåŠŸ (DiskANN ç´¢å¼•)")
    
    # æ’å…¥æ•°æ®
    print("\n" + "=" * 70)
    print(f"ğŸš€ å¼€å§‹æ’å…¥æ•°æ® ({num_vectors:,} æ¡)...")
    print("=" * 70)
    
    batch_size = args.batch_size
    insert_start = time.time()
    
    for i in range(0, num_vectors, batch_size):
        end = min(i + batch_size, num_vectors)
        
        # åŠ è½½æ‰¹æ¬¡æ•°æ®
        if xb is not None:
            # HDF5 æ¨¡å¼ï¼šä»å†…å­˜è¯»å–
            batch_vectors = xb[i:end]
        else:
            # bvecs æ¨¡å¼ï¼šä»æ–‡ä»¶è¯»å–
            batch_vectors = read_bvecs_batch(BVECS_BASE_PATH, i, end - i, DIMENSION)
        
        batch_data = [
            {"id": i + j, "vector": batch_vectors[j].tolist()}
            for j in range(len(batch_vectors))
        ]
        
        client.insert(collection_name=COLLECTION_NAME, data=batch_data)
        
        progress = (end / num_vectors) * 100
        elapsed = time.time() - insert_start
        rate = end / elapsed if elapsed > 0 else 0
        eta = (num_vectors - end) / rate if rate > 0 else 0
        
        print(f"   è¿›åº¦: {end:>12,} / {num_vectors:,} ({progress:5.1f}%) | "
              f"{rate:,.0f} vec/s | ETA: {eta/60:.1f} min", end="\r")
    
    insert_time = time.time() - insert_start
    print(f"\nâœ… æ•°æ®æ’å…¥å®Œæˆ! è€—æ—¶: {insert_time:.1f}s ({insert_time/60:.1f} min)")
    print(f"   æ’å…¥é€Ÿåº¦: {num_vectors / insert_time:,.0f} æ¡/ç§’")
    
    # æ„å»ºç´¢å¼•
    print("\n" + "=" * 70)
    print("ğŸ”¨ æ­£åœ¨æ„å»º DiskANN ç´¢å¼•...")
    print("=" * 70)
    print("   â³ è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    index_start = time.time()
    client.flush(collection_name=COLLECTION_NAME)
    
    while True:
        info = client.describe_index(collection_name=COLLECTION_NAME, index_name="vector_index")
        indexed = info.get('indexed_rows', 0)
        total = info.get('total_rows', num_vectors)
        
        if indexed >= num_vectors:
            break
        
        print(f"   ç´¢å¼•è¿›åº¦: {indexed:,} / {total:,} ({indexed/total*100:.1f}%)", end="\r")
        time.sleep(5)
    
    index_time = time.time() - index_start
    print(f"\nâœ… ç´¢å¼•æ„å»ºå®Œæˆ! è€—æ—¶: {index_time:.1f}s ({index_time/60:.1f} min)")
    
    # åŠ è½½é›†åˆ
    print("\nğŸ“¥ åŠ è½½é›†åˆåˆ°å†…å­˜...")
    client.load_collection(COLLECTION_NAME)
    print("   âœ… é›†åˆå·²åŠ è½½")
    
    # æ€§èƒ½æµ‹è¯•
    TOP_K = 100
    SEARCH_PARAMS = {"metric_type": "L2", "params": {"search_list": 150}}
    NUM_QUERIES = min(1000, len(xq))
    
    print("\n" + "=" * 70)
    print("âš¡ å¼€å§‹æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    print(f"   TopK: {TOP_K}")
    print(f"   search_list: 150")
    print(f"   æµ‹è¯•æŸ¥è¯¢æ•°: {NUM_QUERIES}")
    
    # é¢„çƒ­
    print("\n   ğŸ”¥ é¢„çƒ­ä¸­...")
    for _ in range(10):
        client.search(
            collection_name=COLLECTION_NAME,
            data=[xq[0].tolist()],
            limit=TOP_K,
            search_params=SEARCH_PARAMS
        )
    print("   âœ… é¢„çƒ­å®Œæˆ")
    
    # æ­£å¼æµ‹è¯•
    print("\n   ğŸ” æ‰§è¡Œæœç´¢æµ‹è¯•...")
    query_vectors = [vec.tolist() for vec in xq[:NUM_QUERIES]]
    
    start_time = time.time()
    results = client.search(
        collection_name=COLLECTION_NAME,
        data=query_vectors,
        limit=TOP_K,
        search_params=SEARCH_PARAMS
    )
    end_time = time.time()
    
    # ç»“æœåˆ†æ
    total_time = end_time - start_time
    qps = NUM_QUERIES / total_time
    avg_latency = (total_time / NUM_QUERIES) * 1000
    
    # è®¡ç®—å¬å›ç‡
    recall_rate = None
    if gt is not None:
        recall_count = 0
        for i, hits in enumerate(results):
            result_ids = set([hit['id'] for hit in hits])
            gt_ids = set(gt[i, :TOP_K])
            recall_count += len(result_ids.intersection(gt_ids))
        recall_rate = recall_count / (NUM_QUERIES * TOP_K)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ")
    print("=" * 70)
    print(f"   {'æŒ‡æ ‡':<20} {'ç»“æœ':<20}")
    print("-" * 70)
    print(f"   {'å‘é‡æ€»æ•°':<20} {num_vectors:,}")
    print(f"   {'QPS (ååé‡)':<20} {qps:.2f} queries/sec")
    print(f"   {'å¹³å‡å»¶è¿Ÿ':<20} {avg_latency:.2f} ms/query")
    if recall_rate:
        print(f"   {'Recall@' + str(TOP_K):<20} {recall_rate:.4f} ({recall_rate*100:.2f}%)")
    print("-" * 70)
    print(f"   {'æ’å…¥è€—æ—¶':<20} {insert_time/60:.1f} min")
    print(f"   {'ç´¢å¼•è€—æ—¶':<20} {index_time/60:.1f} min")
    print("=" * 70)
    
    # search_list å‚æ•°å¯¹æ¯”
    print("\nğŸ“ˆ search_list å‚æ•°å¯¹æ¯”æµ‹è¯•:")
    print("-" * 50)
    print(f"{'search_list':<15} {'QPS':<15} {'Recall@100':<15}")
    print("-" * 50)
    
    for sl in [100, 150, 200, 300]:
        params = {"metric_type": "L2", "params": {"search_list": sl}}
        
        start = time.time()
        res = client.search(
            collection_name=COLLECTION_NAME,
            data=query_vectors[:100],
            limit=TOP_K,
            search_params=params
        )
        elapsed = time.time() - start
        
        recall = "N/A"
        if gt is not None:
            recall_cnt = 0
            for i, hits in enumerate(res):
                result_ids = set([hit['id'] for hit in hits])
                gt_ids = set(gt[i, :TOP_K])
                recall_cnt += len(result_ids.intersection(gt_ids))
            recall = f"{recall_cnt / (100 * TOP_K):.4f}"
        
        qps_test = 100 / elapsed
        print(f"{sl:<15} {qps_test:<15.2f} {recall:<15}")
    
    print("-" * 50)
    print("ğŸ’¡ ç»“è®º: search_list è¶Šå¤§ï¼Œå¬å›ç‡è¶Šé«˜ï¼Œä½† QPS ä¼šé™ä½")
    
    # æ¸…ç†
    print("\nğŸ§¹ æ¸…ç†èµ„æº...")
    client.release_collection(COLLECTION_NAME)
    client.close()
    print("âœ… æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
